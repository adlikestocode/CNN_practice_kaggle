{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adityavkini/classifying-x-ray-images-of-normal-and-pneumonia-p?scriptVersionId=218575981\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Classifying X-ray Images of Normal and Pneumonia Patients Using Convolutional Neural Networks","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In this project, we explored the use of **Convolutional Neural Networks (CNNs)** for **image classification**, with a focus on optimizing model architecture and training strategies to achieve better accuracy and generalization. The goal was to classify images into categories based on the features that the CNN automatically learns during the training process. CNNs are particularly well-suited for image-related tasks because they can capture spatial relationships in data, allowing the model to recognize patterns in images effectively.\n\nTo make the process smoother and more effective, we incorporated various strategies, including regularization, dynamic learning rate adjustments, and early stopping. We compared the results of different model configurations to understand how each component affected the model's performance, especially with respect to overfitting and convergence speed.\n\nThrough this topic, we've learned a lot about how CNNs work to classify images and the importance of tuning both the architecture and training strategies for optimal performance. We used a CNN to process and classify images based on features automatically learned by convolutional layers, which are particularly effective at capturing spatial hierarchies in image data. \n\nOur CNN architecture consisted of multiple convolutional layers followed by **MaxPooling** to reduce dimensionality and capture the most relevant features. After flattening the output, we added fully connected layers to make the final classification decision. The integration of **BatchNormalization** helped stabilize and speed up training, and **Dropout** regularization effectively prevented overfitting by randomly disabling neurons during training. We also employed techniques like **EarlyStopping**, which halted training once the validation loss ceased improving, and **ReduceLROnPlateau**, which dynamically adjusted the learning rate for more efficient convergence.\n\nIn comparing two different model setups, we saw a significant difference in their performance. The first model, with its well-optimized architecture and training approach, demonstrated steady improvements in accuracy and a strong generalization ability on the validation set. In contrast, the second model, despite similar architecture, struggled with slower convergence and higher validation loss due to a lack of regularization and efficient training callbacks. \n\nFrom this, we learned that achieving strong image classification results with CNNs requires more than just the right architecture. The training strategy—such as using proper regularization, callbacks, and dynamic learning rate adjustments—plays a crucial role in determining the model's ability to generalize effectively. The key takeaway is that combining a well-designed model with thoughtful training tactics can lead to a more robust and efficient performance on unseen data.","metadata":{}},{"cell_type":"markdown","source":"# Counting Images in Each Folder for Dataset Distribution","metadata":{}},{"cell_type":"markdown","source":"This section of the code counts the number of images in each folder of a given directory. It is useful for understanding the distribution of images in different subfolders, especially when working with datasets that contain multiple categories.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\ndef count_images_in_folders(directory, valid_extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")):\n    folder_image_counts = {}\n    \n    for dirname, _, filenames in os.walk(directory):\n        # Count images with valid extensions in the current folder\n        image_count = sum(1 for filename in filenames if filename.lower().endswith(valid_extensions))\n        folder_image_counts[dirname] = image_count\n\n    return folder_image_counts\n\n# Directory to inspect (replace with your dataset directory)\nbase_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n\n# Get image counts\nimage_counts = count_images_in_folders(base_dir)\n\n# Print results\nfor folder, count in image_counts.items():\n    print(f\"Folder: {folder} - Number of images: {count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T16:14:37.06348Z","iopub.execute_input":"2025-01-20T16:14:37.063819Z","iopub.status.idle":"2025-01-20T16:14:44.593523Z","shell.execute_reply.started":"2025-01-20T16:14:37.063788Z","shell.execute_reply":"2025-01-20T16:14:44.591921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Essential Libraries for Data Processing and Model Building","metadata":{}},{"cell_type":"markdown","source":"This section includes the essential libraries imported for loading, processing, and augmenting image data, as well as building and training a neural network model. Each of these libraries plays a crucial role in your deep learning workflow.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:41:49.246358Z","iopub.execute_input":"2025-01-20T19:41:49.246768Z","iopub.status.idle":"2025-01-20T19:41:49.25195Z","shell.execute_reply.started":"2025-01-20T19:41:49.246739Z","shell.execute_reply":"2025-01-20T19:41:49.250844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This section demonstrates how to count the number of images in various datasets and visualize the results. By utilizing the count_images_in_directory function, we can easily assess the dataset's distribution and identify any discrepancies. After noticing that the validation set is underrepresented, we plan to modify the dataset by moving images from the training set to the validation set for better balance.","metadata":{}},{"cell_type":"code","source":"# Function to count images\ndef count_images_in_directory(directory, valid_extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")):\n    count = 0\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(valid_extensions):\n                count += 1\n    return count\n\n# Paths\ntrain_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/train'\nval_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val'\ntest_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test'\n\n# Image counts\ntrain_images_count = count_images_in_directory(train_dir)\nval_images_count = count_images_in_directory(val_dir)\ntest_images_count = count_images_in_directory(test_dir)\n\n# Visualization of image counts\ncounts = [train_images_count, val_images_count, test_images_count]\nlabels = ['Train', 'Validation', 'Test']\nplt.bar(labels, counts, color=['blue', 'orange', 'green'])\nplt.title('Image Counts in Dataset')\nplt.ylabel('Number of Images')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:33:16.616219Z","iopub.execute_input":"2025-01-20T19:33:16.616631Z","iopub.status.idle":"2025-01-20T19:33:21.619647Z","shell.execute_reply.started":"2025-01-20T19:33:16.6166Z","shell.execute_reply":"2025-01-20T19:33:21.618478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing Dataset Distribution and Adjusting for Balance","metadata":{}},{"cell_type":"markdown","source":"This section focuses on adjusting the image distribution between the training and validation datasets. The split_train_to_validation_copy function is used to move a portion of images from the training set to the validation set, ensuring a more balanced distribution. After performing this split, the image counts are recalculated and visualized to confirm the adjustments.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\ndef split_train_to_validation_copy(train_dir, val_dir, split_ratio=0.2, valid_extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")):\n    \"\"\"\n    Splits a portion of images from the training directory into the validation directory by copying.\n    \n    Parameters:\n    - train_dir: Path to the training directory.\n    - val_dir: Path to the validation directory.\n    - split_ratio: Proportion of the training data to copy to validation.\n    - valid_extensions: Valid image file extensions.\n    \"\"\"\n    if not os.path.exists(val_dir):\n        os.makedirs(val_dir)\n    \n    for class_name in os.listdir(train_dir):\n        class_train_path = os.path.join(train_dir, class_name)\n        class_val_path = os.path.join(val_dir, class_name)\n        \n        # Skip if not a directory\n        if not os.path.isdir(class_train_path):\n            continue\n        \n        # Create validation class directory if it doesn't exist\n        if not os.path.exists(class_val_path):\n            os.makedirs(class_val_path)\n        \n        # Get all valid image files\n        image_files = [f for f in os.listdir(class_train_path) if f.lower().endswith(valid_extensions)]\n        \n        # Determine how many images to copy\n        num_to_copy = int(len(image_files) * split_ratio)\n        \n        # Randomly select images to copy\n        images_to_copy = random.sample(image_files, num_to_copy)\n        \n        # Copy selected images to validation directory\n        for image in images_to_copy:\n            src_path = os.path.join(class_train_path, image)\n            dest_path = os.path.join(class_val_path, image)\n            shutil.copy(src_path, dest_path)\n        \n        print(f\"Copied {num_to_copy} images from {class_train_path} to {class_val_path}\")\n\n# Define temporary writable directories\nwritable_train_dir = '/kaggle/working/train'\nwritable_val_dir = '/kaggle/working/val'\n\n# Copy the dataset to writable directories\nif not os.path.exists(writable_train_dir):\n    shutil.copytree(train_dir, writable_train_dir)\n\n# Perform the split\nsplit_train_to_validation_copy(writable_train_dir, writable_val_dir, split_ratio=0.2)\n\n# Paths\ntrain_dir = '/kaggle/working/train'\nval_dir = '/kaggle/working/val'\ntest_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test'\n\n# Image counts\ntrain_images_count = count_images_in_directory(train_dir)\nval_images_count = count_images_in_directory(val_dir)\ntest_images_count = count_images_in_directory(test_dir)\n\n# Visualization of image counts\ncounts = [train_images_count, val_images_count, test_images_count]\nlabels = ['Train', 'Validation', 'Test']\nplt.bar(labels, counts, color=['blue', 'orange', 'green'])\nplt.title('Updated Image Counts in Dataset')\nplt.ylabel('Number of Images')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:33:24.85337Z","iopub.execute_input":"2025-01-20T19:33:24.853811Z","iopub.status.idle":"2025-01-20T19:33:26.393105Z","shell.execute_reply.started":"2025-01-20T19:33:24.853735Z","shell.execute_reply":"2025-01-20T19:33:26.392103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting Up Data Augmentation and Generating Data Batches","metadata":{}},{"cell_type":"markdown","source":"In this section, we set up data augmentation for the training dataset to improve model generalization and variability. A variety of transformations, including rotation, width and height shifts, shear, zoom, flipping, brightness adjustment, and channel shift, are applied to enhance the diversity of the training data. Additionally, the dataset is split into training and validation sets using the validation_split parameter.\n\nData generators are created for the training, validation, and test datasets, where images are resized and rescaled, and batches are prepared for the model training. The number of images in each set and their corresponding class names are printed to ensure everything is set up correctly.","metadata":{}},{"cell_type":"code","source":"# Define the directories for training, validation, and testing\ntrain_dir = '/kaggle/working/train'  # Update to the path where your remaining training images are\nval_dir = '/kaggle/working/val'      # Update to the path where your validation images (including transferred ones) are\ntest_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/test'    # Test directory remains the same\n\n# Data Augmentation for training\ndata_gen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=30,         # Increased rotation range for more variation\n    width_shift_range=0.3,     # Increased width shift range\n    height_shift_range=0.3,    # Increased height shift range\n    shear_range=0.3,           # Increased shear range\n    zoom_range=0.3,            # Increased zoom range\n    horizontal_flip=True,\n    vertical_flip=True,        # Added vertical flip\n    brightness_range=[0.5, 1.5], # Added brightness variation\n    channel_shift_range=20.0,  # Added channel shift range for color adjustments\n    fill_mode='nearest',\n    validation_split=0.2       # This ensures 20% of the training data is used for validation\n)\n\n# Data Generators for training and validation\ntrain_generator = data_gen.flow_from_directory(\n    train_dir,  # This is the directory with your remaining training data\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='training',  # Use 80% of train_dir for training\n    shuffle=True\n)\n\n# Print the class names and directory path for training data\nprint(f\"Training Data Directory: {train_dir}\")\nprint(f\"Class Names: {list(train_generator.class_indices.keys())}\")\nprint(f\"Total Images in Training Set: {train_generator.samples}\")\nprint(\"-\" * 50)\n\nval_generator = data_gen.flow_from_directory(\n    val_dir,  # This is the new validation directory\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation',  # Use 20% of the val_dir for validation\n    shuffle=True\n)\n\n# Print the class names and directory path for validation data\nprint(f\"Validation Data Directory: {val_dir}\")\nprint(f\"Class Names: {list(val_generator.class_indices.keys())}\")\nprint(f\"Total Images in Validation Set: {val_generator.samples}\")\nprint(\"-\" * 50)\n\n# For testing data (no augmentation here)\ntest_datagen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,  # Test directory remains unchanged\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    shuffle=True\n)\n\n# Print the class names and directory path for testing data\nprint(f\"Test Data Directory: {test_dir}\")\nprint(f\"Class Names: {list(test_generator.class_indices.keys())}\")\nprint(f\"Total Images in Test Set: {test_generator.samples}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T18:18:51.994717Z","iopub.execute_input":"2025-01-20T18:18:51.99542Z","iopub.status.idle":"2025-01-20T18:18:52.42697Z","shell.execute_reply.started":"2025-01-20T18:18:51.995386Z","shell.execute_reply":"2025-01-20T18:18:52.425967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing Augmented Images from the Training Dataset","metadata":{}},{"cell_type":"markdown","source":"In this section, we visualize a few sample images from the training dataset after applying the data augmentation transformations. By selecting a batch from the training generator, we display 9 images in a 3x3 grid. These images demonstrate the variations introduced by the augmentation process, such as rotation, shifting, flipping, and other transformations, which help increase the model's robustness to different data scenarios.\n\nThis step is crucial for visually confirming that the augmentations are working as expected.","metadata":{}},{"cell_type":"code","source":"# Display a few sample images\nsample_batch = next(train_generator)\nsample_images, _ = sample_batch\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(sample_images[i])\n    plt.axis('off')\nplt.suptitle('Sample Images After Augmentation', fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T18:18:57.604865Z","iopub.execute_input":"2025-01-20T18:18:57.605336Z","iopub.status.idle":"2025-01-20T18:18:59.091299Z","shell.execute_reply.started":"2025-01-20T18:18:57.605294Z","shell.execute_reply":"2025-01-20T18:18:59.090266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this section, a simple Convolutional Neural Network (CNN) model is defined using Keras' Sequential API. The architecture includes:\n\nConv2D Layers: These convolutional layers (32 and 64 filters with 3x3 kernels) extract features from the input images.\nMaxPooling2D Layers: Pooling layers reduce the spatial dimensions of the feature maps, helping to control overfitting.\nFlatten Layer: This layer flattens the output of the convolutional layers to a 1D vector for input into the fully connected layers.\nDense Layers: A fully connected layer with 128 units and a ReLU activation function to learn complex patterns, followed by a dropout layer (with a 50% drop probability) to reduce overfitting.\nFinal Dense Layer: A single unit with a sigmoid activation function for binary classification (pneumonia vs. normal).\nThe model is compiled using the Adam optimizer, binary cross-entropy loss, and accuracy as the evaluation metric.","metadata":{}},{"cell_type":"code","source":"# Model Definition (example, customize as per your needs)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T18:23:25.272867Z","iopub.execute_input":"2025-01-20T18:23:25.273222Z","iopub.status.idle":"2025-01-20T18:23:25.387209Z","shell.execute_reply.started":"2025-01-20T18:23:25.273193Z","shell.execute_reply":"2025-01-20T18:23:25.386323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining and Compiling the CNN Model Architecture","metadata":{}},{"cell_type":"markdown","source":"In this section, the model is trained using the fit method. The key steps include:\n\nTraining Data: The model is trained using train_generator, which provides augmented image data from the training set.\nValidation Data: The validation data is provided by val_generator, which supplies images from the validation set.\nEpochs: The model will train for 10 epochs.\nSteps per Epoch: The number of steps per epoch is calculated as the total number of training samples divided by the batch size.\nValidation Steps: Similarly, the validation steps are calculated based on the number of validation samples and batch size.\nThis training process helps the model learn to classify images, while the validation data ensures the model's generalization.\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Model Training\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    validation_data=val_generator,\n    validation_steps=val_generator.samples // val_generator.batch_size,\n    epochs=10\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This model was trained over 10 epochs using a convolutional neural network (CNN) for binary classification. The following summarizes the performance of the model throughout the training process.\n\nKey Metrics:\nTraining Accuracy: Represents how well the model performed on the training data.\nValidation Accuracy: Represents the model's performance on unseen validation data.\nTraining Loss: The error on the training data. Lower values indicate better performance.\nValidation Loss: The error on the validation data. Again, lower values are preferable.\nEpoch-by-Epoch Performance:\nEpoch 1:\n\nTraining Accuracy: 77.10%\nValidation Accuracy: 82.10%\nTraining Loss: 0.4412\nValidation Loss: 0.3891\nModel starts well, with relatively good accuracy on both training and validation data.\n\nEpoch 2:\n\nTraining Accuracy: 75.00%\nValidation Accuracy: 72.00%\nTraining Loss: 0.4766\nValidation Loss: 0.4427\nSlight drop in performance, especially in validation accuracy and loss.\n\nEpoch 3:\n\nTraining Accuracy: 80.74%\nValidation Accuracy: 85.80%\nTraining Loss: 0.3806\nValidation Loss: 0.3005\nImprovement in both training and validation accuracy, with a decrease in loss.\n\nEpoch 4:\n\nTraining Accuracy: 81.25%\nValidation Accuracy: 72.00%\nTraining Loss: 0.4902\nValidation Loss: 0.5081\nPerformance drops again, particularly in validation accuracy, while training loss increases.\n\nEpoch 5:\n\nTraining Accuracy: 80.75%\nValidation Accuracy: 83.81%\nTraining Loss: 0.3862\nValidation Loss: 0.3512\nRecovery in validation accuracy, but training accuracy remains stable.\n\nEpoch 6:\n\nTraining Accuracy: 84.38%\nValidation Accuracy: 88.00%\nTraining Loss: 0.3154\nValidation Loss: 0.2347\nThe best performance so far! Both training and validation accuracy peak, and validation loss is at its lowest.\n\nEpoch 7:\n\nTraining Accuracy: 81.27%\nValidation Accuracy: 81.53%\nTraining Loss: 0.3712\nValidation Loss: 0.3987\nA slight decrease in performance in both accuracy and loss, with some fluctuation in validation results.\n\nEpoch 8:\n\nTraining Accuracy: 87.50%\nValidation Accuracy: 84.00%\nTraining Loss: 0.4023\nValidation Loss: 0.3351\nTraining accuracy improves, but validation accuracy stays stable, and the validation loss decreases slightly.\n\nEpoch 9:\n\nTraining Accuracy: 82.30%\nValidation Accuracy: 82.95%\nTraining Loss: 0.3710\nValidation Loss: 0.3523\nThe model performance stabilizes, with no significant improvement or deterioration in both training and validation metrics.\n\nEpoch 10:\n\nTraining Accuracy: 75.00%\nValidation Accuracy: 80.00%\nTraining Loss: 0.4157\nValidation Loss: 0.3710\nA slight decrease in accuracy and an increase in loss towards the end of training, indicating potential overfitting.\n\nKey Takeaways:\nBest Performance: The model performed best in Epoch 6, with 88% validation accuracy and lowest validation loss (0.2347).\nFluctuations in Performance: The training accuracy fluctuated over the epochs, and validation accuracy showed some drops, especially after Epoch 6. This indicates potential overfitting, where the model starts to memorize the training data and doesn't generalize well to unseen data.\nOverfitting Indicators: After Epoch 6, we observe training accuracy increasing but validation accuracy stagnating or decreasing, a sign of overfitting.\nConclusion:\nThe model showed promising results, with peak validation accuracy of 88% in the middle epochs.\nWe can improve the model by adding more regularization, adjusting the learning rate, or training for more epochs.","metadata":{}},{"cell_type":"markdown","source":"# Visualizing Model Training Performance and Saving Results","metadata":{}},{"cell_type":"markdown","source":"This section visualizes the model's training performance and saves the results:\n\nTraining and Validation Accuracy: The accuracy over each epoch for both training and validation datasets is plotted. This provides insights into how well the model is learning over time.\nTraining and Validation Loss: The loss values for training and validation are also plotted, which helps track how well the model is minimizing the error across epochs.\nGraph Details:\nThe training and validation accuracy are shown in one plot.\nThe training and validation loss are shown in a separate plot.\nResults Export: After training, the accuracy and loss data for each epoch are saved into a CSV file (training_results.csv) for future analysis.","metadata":{}},{"cell_type":"code","source":"# Training Visualization\ntrain_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(train_accuracy) + 1)\n\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_accuracy, label='Training Accuracy', marker='o')\nplt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\nplt.title('Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss, label='Training Loss', marker='o', color='red')\nplt.plot(epochs, val_loss, label='Validation Loss', marker='o', color='purple')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n# Save Final Results\nresults_df = pd.DataFrame({\n    'Epoch': epochs,\n    'Training Accuracy': train_accuracy,\n    'Validation Accuracy': val_accuracy,\n    'Training Loss': train_loss,\n    'Validation Loss': val_loss\n})\nresults_df.to_csv('training_results.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:14:42.147932Z","iopub.execute_input":"2025-01-20T19:14:42.148362Z","iopub.status.idle":"2025-01-20T19:14:43.244873Z","shell.execute_reply.started":"2025-01-20T19:14:42.148326Z","shell.execute_reply":"2025-01-20T19:14:43.243783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluating Model Performance: Classification Report and Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"This section evaluates the model's performance on the test dataset by generating the following:\n\nClassification Report: This report includes key metrics such as precision, recall, f1-score, and accuracy for each class. It provides a detailed assessment of how well the model performs across different categories.\n\nConfusion Matrix: This matrix visualizes the number of correct and incorrect predictions for each class. The rows represent the true labels, and the columns represent the predicted labels. It's displayed using a heatmap for easier interpretation.\n\nTextual Output: In addition to the confusion matrix visualization, the matrix values are printed as raw numbers for further inspection.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Predict on the test set and get the probabilities\ntest_predictions = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n\n# Convert predictions to binary values (0 or 1) based on the threshold of 0.5\ntest_predictions = np.round(test_predictions).astype(int)\n\n# Get the true labels (from the generator)\ntrue_labels = test_generator.classes\n\n# Ensure the length of true labels and predictions match\nassert len(true_labels) == len(test_predictions), \"Mismatch between true labels and predictions length.\"\n\n# Classification Report\nprint(\"\\nClassification Report:\\n\")\nreport = classification_report(true_labels, test_predictions, target_names=test_generator.class_indices.keys())\nprint(report)\n\n# Confusion Matrix\ncm = confusion_matrix(true_labels, test_predictions)\n\n# Plotting Confusion Matrix using Seaborn heatmap\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Additional text output for the confusion matrix\nprint(\"\\nConfusion Matrix (True vs Predicted):\")\nprint(cm)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Improvements: Architecture Changes, Optimization, and Callbacks\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"\nTo improve the accuracy of our model, we will be incorporating several changes to the existing architecture. Here’s a summary of the changes:\nConvolutional Layers:\n\nThree convolutional layers with increasing filter sizes (32, 64, 128) to capture more complex features.\nBatchNormalization after each convolutional layer to stabilize training and improve generalization.\nMaxPooling layers to reduce spatial dimensions and retain important features.\nFully Connected Layers:\n\nIncreased the number of units in the dense layers (512 and 256) to enable the model to learn more complex patterns.\nAdded Dropout (0.5) in the dense layers to prevent overfitting.\nOptimization and Learning Rate Scheduling:\n\nUsed Adam optimizer with an initial learning rate of 0.001 to efficiently train the model.\nIncorporated Learning Rate Scheduling with ReduceLROnPlateau to adjust the learning rate if the validation loss plateaus.\nCallbacks:\n\nEarlyStopping to halt training early if the validation loss does not improve for 5 consecutive epochs, preventing overfitting and ensuring the best model weights.\nThese adjustments are designed to improve the model's performance by enabling it to learn better feature representations and adaptively adjust the training process.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Define the Model\nmodel = Sequential([\n    # Input layer\n    Input(shape=(150, 150, 3)),\n\n    # Convolutional Layer 1\n    Conv2D(32, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    # Convolutional Layer 2\n    Conv2D(64, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    # Convolutional Layer 3\n    Conv2D(128, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    # Flatten the output\n    Flatten(),\n\n    # Fully Connected Layer 1\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n\n    # Fully Connected Layer 2 (Increased capacity)\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n\n    # Output layer\n    Dense(1, activation='sigmoid')  # For binary classification\n])\n\n# Compile the model with Adam optimizer and learning rate scheduler\ninitial_learning_rate = 0.001\noptimizer = Adam(learning_rate=initial_learning_rate)\n\n# Add Learning Rate Scheduling\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n\n# Compile the model\nmodel.compile(optimizer=optimizer,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# EarlyStopping to avoid overfitting\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Model Training with callbacks\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    validation_data=val_generator,\n    validation_steps=val_generator.samples // val_generator.batch_size,\n    epochs=10,\n    callbacks=[early_stopping, lr_scheduler]  # Adding EarlyStopping and Learning Rate Scheduler\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T18:45:59.704824Z","iopub.execute_input":"2025-01-20T18:45:59.705252Z","iopub.status.idle":"2025-01-20T19:05:40.935981Z","shell.execute_reply.started":"2025-01-20T18:45:59.705213Z","shell.execute_reply":"2025-01-20T19:05:40.934741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The first model, with added enhancements like BatchNormalization, Dropout, and EarlyStopping, demonstrates consistent improvements in both training and validation accuracy. It begins with a strong validation accuracy of 82.10% in the first epoch and maintains solid performance throughout, reaching up to 88.00% validation accuracy in later epochs.\n\nIn contrast, the second model shows lower initial performance with training accuracy of 71.88% and high validation loss in the first epoch. Although some improvement is seen, its performance lags behind, with validation accuracy maxing out at 76.42% by epoch 6. Additionally, the second model experiences higher training and validation loss compared to the original model.\n\nOverall, the first model performs better, especially in validation accuracy, and is more stable in generalization.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Predict on the test set and get the probabilities\ntest_predictions = model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n\n# Convert predictions to binary values (0 or 1) based on the threshold of 0.5\ntest_predictions = np.round(test_predictions).astype(int)\n\n# Get the true labels (from the generator)\ntrue_labels = test_generator.classes\n\n# Ensure the length of true labels and predictions match\nassert len(true_labels) == len(test_predictions), \"Mismatch between true labels and predictions length.\"\n\n# Classification Report\nprint(\"\\nClassification Report:\\n\")\nreport = classification_report(true_labels, test_predictions, target_names=test_generator.class_indices.keys())\nprint(report)\n\n# Confusion Matrix\ncm = confusion_matrix(true_labels, test_predictions)\n\n# Plotting Confusion Matrix using Seaborn heatmap\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Additional text output for the confusion matrix\nprint(\"\\nConfusion Matrix (True vs Predicted):\")\nprint(cm)\n# Print training and validation accuracy\nprint(\"Training Accuracy: \", history.history['accuracy'][-1])\nprint(\"Validation Accuracy: \", history.history['val_accuracy'][-1])\n\n# Print the test accuracy\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T19:18:07.688091Z","iopub.execute_input":"2025-01-20T19:18:07.688523Z","iopub.status.idle":"2025-01-20T19:18:18.825715Z","shell.execute_reply.started":"2025-01-20T19:18:07.688489Z","shell.execute_reply":"2025-01-20T19:18:18.824822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Visualization\ntrain_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(train_accuracy) + 1)\n\nplt.figure(figsize=(14, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_accuracy, label='Training Accuracy', marker='o')\nplt.plot(epochs, val_accuracy, label='Validation Accuracy', marker='o')\nplt.title('Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_loss, label='Training Loss', marker='o', color='red')\nplt.plot(epochs, val_loss, label='Validation Loss', marker='o', color='purple')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n# Save Final Results\nresults_df = pd.DataFrame({\n    'Epoch': epochs,\n    'Training Accuracy': train_accuracy,\n    'Validation Accuracy': val_accuracy,\n    'Training Loss': train_loss,\n    'Validation Loss': val_loss\n})\nresults_df.to_csv('training_results.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Through this topic, we've learned a lot about how **Convolutional Neural Networks (CNNs)** work to classify images and the importance of tuning both the architecture and training strategies for optimal performance. We used a CNN to process and classify images based on features automatically learned by convolutional layers, which are particularly effective at capturing spatial hierarchies in image data. \n\nOur CNN architecture consisted of multiple convolutional layers followed by **MaxPooling** to reduce dimensionality and capture the most relevant features. After flattening the output, we added fully connected layers to make the final classification decision. The integration of **BatchNormalization** helped stabilize and speed up training, and **Dropout** regularization effectively prevented overfitting by randomly disabling neurons during training. We also employed techniques like **EarlyStopping**, which halted training once the validation loss ceased improving, and **ReduceLROnPlateau**, which dynamically adjusted the learning rate for more efficient convergence.\n\nIn comparing two different model setups, we saw a significant difference in their performance. The first model, with its well-optimized architecture and training approach, demonstrated steady improvements in accuracy and a strong generalization ability on the validation set. In contrast, the second model, despite similar architecture, struggled with slower convergence and higher validation loss due to a lack of regularization and efficient training callbacks. \n\nFrom this, we learned that achieving strong image classification results with CNNs requires more than just the right architecture. The training strategy—such as using proper regularization, callbacks, and dynamic learning rate adjustments—plays a crucial role in determining the model's ability to generalize effectively. The key takeaway is that combining a well-designed model with thoughtful training tactics can lead to a more robust and efficient performance on unseen data.","metadata":{}}]}